{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the Young People Survey dataset (https://www.kaggle.com/miroslavsabo/young-people-survey/) to predict a person’s “empathy” on a scale from 1 to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us load the dataset and split it into different groups to make it more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "print(\"Reading data...\")\n",
    "df = pd.read_csv(\"responses.csv\")\n",
    "columns = pd.read_csv(\"columns.csv\")\n",
    "\n",
    "music = df.iloc[:,0:19]\n",
    "movies = df.iloc[:,19:31]\n",
    "hobbies = df.iloc[:,31:63]\n",
    "phobias = df.iloc[:,63:73]\n",
    "health = df.iloc[:,73:76]\n",
    "personality = df.iloc[:, 76:133]\n",
    "spending = df.iloc[:,133:140]\n",
    "demographics = df.iloc[:,140:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 11 categorical features in this data. Let us convert them to numerical features in order to use them through sklearn library. We have to also fill in any missing values in the data for better predictions. We can do this by replacing the missing values by the most frequent values of that respective column in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "['never smoked' 'tried smoking' 'former smoker' 'current smoker' nan]\n",
      "['drink a lot' 'social drinker' 'never' nan]\n",
      "['i am always on time' 'i am often early' 'i am often running late' nan]\n",
      "['never' 'sometimes' 'only to avoid hurting someone'\n",
      " 'everytime it suits me' nan]\n",
      "['few hours a day' 'most of the day' 'less than an hour a day'\n",
      " 'no time at all']\n",
      "['college/bachelor degree' 'secondary school' 'primary school'\n",
      " 'masters degree' 'doctorate degree' 'currently a primary school pupil'\n",
      " nan]\n",
      "['no' 'yes' nan]\n",
      "['village' 'city' nan]\n",
      "['block of flats' 'house/bungalow' nan]\n",
      "30.0\n",
      "15.0\n",
      "203.0\n",
      "62.0\n",
      "165.0\n",
      "41.0\n",
      "[ 1.  2.  3. 10.  0.  4.  5. nan  6.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing data...\")\n",
    "music = music.replace(\"nan\", np.nan)\n",
    "music = music.replace(\"NaN\", np.nan)\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "imp.fit(music)\n",
    "music_data = imp.transform(music)\n",
    "music = pd.DataFrame(data=music_data[:,:],\n",
    "                     index=[i for i in range(len(music_data))],\n",
    "                     columns=music.columns.tolist())\n",
    "\n",
    "movies = movies.replace(\"nan\", np.nan)\n",
    "movies = movies.replace(\"NaN\", np.nan)\n",
    "\n",
    "imp.fit(movies)\n",
    "movies_data = imp.transform(movies)\n",
    "movies = pd.DataFrame(data=movies_data[:,:],\n",
    "                     index=[i for i in range(len(movies_data))],\n",
    "                     columns=movies.columns.tolist())\n",
    "\n",
    "hobbies = hobbies.replace(\"nan\", np.nan)\n",
    "hobbies = hobbies.replace(\"NaN\", np.nan)\n",
    "\n",
    "imp.fit(hobbies)\n",
    "hobbies_data = imp.transform(hobbies)\n",
    "hobbies = pd.DataFrame(data=hobbies_data[:,:],\n",
    "                     index=[i for i in range(len(hobbies_data))],\n",
    "                     columns=hobbies.columns.tolist())\n",
    "\n",
    "phobias = phobias.replace(\"nan\", np.nan)\n",
    "phobias = phobias.replace(\"NaN\", np.nan)\n",
    "\n",
    "imp.fit(phobias)\n",
    "phobias_data = imp.transform(phobias)\n",
    "phobias = pd.DataFrame(data=phobias_data[:,:],\n",
    "                     index=[i for i in range(len(phobias_data))],\n",
    "                     columns=phobias.columns.tolist())\n",
    "\n",
    "print(health[\"Smoking\"].unique())\n",
    "\n",
    "for i in health[\"Smoking\"]:\n",
    "    if i == \"never smoked\":\n",
    "        health.replace(i, 1.0, inplace=True)\n",
    "    elif i == \"tried smoking\":\n",
    "        health.replace(i, 2.0, inplace=True)\n",
    "    elif i == \"former smoker\":\n",
    "        health.replace(i, 3.0, inplace=True)\n",
    "    elif i == \"current smoker\":\n",
    "        health.replace(i, 4.0, inplace=True)\n",
    "\n",
    "print(health[\"Alcohol\"].unique())\n",
    "for i in health[\"Alcohol\"]:\n",
    "    if i == \"never\":\n",
    "        health.replace(i, 1.0, inplace=True)\n",
    "    elif i == \"social drinker\":\n",
    "        health.replace(i, 2.0, inplace=True)\n",
    "    elif i == \"drink a lot\":\n",
    "        health.replace(i, 3.0, inplace=True)\n",
    "\n",
    "health = health.replace(\"nan\", np.nan)\n",
    "health = health.replace(\"NaN\", np.nan)\n",
    "\n",
    "imp.fit(health)\n",
    "health_data = imp.transform(health)\n",
    "health = pd.DataFrame(data=health_data[:,:],\n",
    "                     index=[i for i in range(len(health_data))],\n",
    "                     columns=health.columns.tolist())\n",
    "\n",
    "print(personality[\"Punctuality\"].unique())\n",
    "for i in personality[\"Punctuality\"]:\n",
    "    if i == \"i am often running late\":\n",
    "        personality.replace(i, 1.0, inplace=True)\n",
    "    elif i == \"i am always on time\":\n",
    "        personality.replace(i, 2.0, inplace=True)\n",
    "    elif i == \"i am often early\":\n",
    "        personality.replace(i, 3.0, inplace=True)\n",
    "\n",
    "print(personality[\"Lying\"].unique())        \n",
    "for i in personality[\"Lying\"]:\n",
    "    if i == \"never\":\n",
    "        personality.replace(i, 1.0, inplace=True)\n",
    "    elif i == \"only to avoid hurting someone\":\n",
    "        personality.replace(i, 2.0, inplace=True)\n",
    "    elif i == \"sometimes\":\n",
    "        personality.replace(i, 3.0, inplace=True)\n",
    "    elif i == \"everytime it suits me\":\n",
    "        personality.replace(i, 4.0, inplace=True)\n",
    "\n",
    "print(personality[\"Internet usage\"].unique())\n",
    "for i in personality[\"Internet usage\"]:\n",
    "    if i == \"no time at all\":\n",
    "        personality.replace(i, 1.0, inplace=True)\n",
    "    elif i == \"less than an hour a day\":\n",
    "        personality.replace(i, 2.0, inplace=True)\n",
    "    elif i == \"few hours a day\":\n",
    "        personality.replace(i, 3.0, inplace=True)\n",
    "    elif i == \"most of the day\":\n",
    "        personality.replace(i, 4.0, inplace=True)\n",
    "\n",
    "personality = personality.replace(\"nan\", np.nan)\n",
    "personality = personality.replace(\"NaN\", np.nan)\n",
    "\n",
    "imp.fit(personality)\n",
    "personality_data = imp.transform(personality)\n",
    "personality = pd.DataFrame(data=personality_data[:,:],\n",
    "                     index=[i for i in range(len(personality_data))],\n",
    "                     columns=personality.columns.tolist())\n",
    "\n",
    "spending = spending.replace(\"nan\", np.nan)\n",
    "spending = spending.replace(\"NaN\", np.nan)\n",
    "\n",
    "imp.fit(spending)\n",
    "spending_data = imp.transform(spending)\n",
    "spending = pd.DataFrame(data=spending_data[:,:],\n",
    "                     index=[i for i in range(len(spending_data))],\n",
    "                     columns=spending.columns.tolist())\n",
    "\n",
    "for i in demographics[\"Gender\"]:\n",
    "    if i == \"female\":\n",
    "        demographics.replace(i, 1.0, inplace=True)\n",
    "    elif i == \"male\":\n",
    "        demographics.replace(i, 2.0, inplace=True)\n",
    "\n",
    "for i in demographics[\"Left - right handed\"]:\n",
    "    if i == \"right handed\":\n",
    "        demographics.replace(i, 1.0, inplace=True)\n",
    "    elif i == \"left handed\":\n",
    "        demographics.replace(i, 2.0, inplace=True)\n",
    "\n",
    "print(demographics[\"Education\"].unique())\n",
    "for i in demographics[\"Education\"]:\n",
    "    if i == \"currently a primary school pupil\":\n",
    "        demographics.replace(i, 1.0, inplace=True)\n",
    "    elif i == \"primary school\":\n",
    "        demographics.replace(i, 2.0, inplace=True)\n",
    "    elif i == \"secondary school\":\n",
    "        demographics.replace(i, 3.0, inplace=True)\n",
    "    elif i == \"college/bachelor degree\":\n",
    "        demographics.replace(i, 4.0, inplace=True)\n",
    "    elif i == \"masters degree\":\n",
    "        demographics.replace(i, 5.0, inplace=True)\n",
    "    elif i == \"doctorate degree\":\n",
    "        demographics.replace(i, 6.0, inplace=True)\n",
    "\n",
    "print(demographics[\"Only child\"].unique())\n",
    "for i in demographics[\"Only child\"]:\n",
    "    if i == \"yes\":\n",
    "        demographics.replace(i, 1.0, inplace=True)\n",
    "    elif i == \"no\":\n",
    "        demographics.replace(i, 2.0, inplace=True)\n",
    "        \n",
    "print(demographics[\"Village - town\"].unique())\n",
    "for i in demographics[\"Village - town\"]:\n",
    "    if i==\"village\":\n",
    "        demographics.replace(i, 1.0, inplace=True)\n",
    "    elif i==\"city\":\n",
    "        demographics.replace(i, 2.0, inplace=True)\n",
    "\n",
    "print(demographics[\"House - block of flats\"].unique())\n",
    "for i in demographics[\"House - block of flats\"]:\n",
    "    if i == \"block of flats\":\n",
    "        demographics.replace(i, 1.0, inplace=True)\n",
    "    elif i == \"house/bungalow\":\n",
    "        demographics.replace(i, 2.0, inplace=True)\n",
    "\n",
    "print(max(demographics[\"Age\"]))\n",
    "print(min(demographics[\"Age\"]))\n",
    "for i in demographics[\"Age\"]:\n",
    "    if (15 <= i < 19):\n",
    "        demographics[\"Age\"].replace(i, 1.0, inplace=True)\n",
    "    elif (19 <= i < 23):\n",
    "        demographics[\"Age\"].replace(i, 2.0, inplace=True)\n",
    "    elif (23 <= i < 27):\n",
    "        demographics[\"Age\"].replace(i, 3.0, inplace=True)\n",
    "    elif (27 <= i < 31):\n",
    "        demographics[\"Age\"].replace(i, 4.0, inplace=True)\n",
    "\n",
    "print(max(demographics[\"Height\"]))\n",
    "print(min(demographics[\"Height\"]))        \n",
    "for i in demographics[\"Height\"]:\n",
    "    if (i >= 180):\n",
    "        demographics[\"Height\"].replace(i, 1.0, inplace=True)\n",
    "    elif (170 <= i < 180):\n",
    "        demographics[\"Height\"].replace(i, 2.0, inplace=True)\n",
    "    elif (160 <= i < 170):\n",
    "        demographics[\"Height\"].replace(i, 3.0, inplace=True)\n",
    "    elif (i < 160):\n",
    "        demographics[\"Height\"].replace(i, 4.0, inplace=True)\n",
    "\n",
    "print(max(demographics[\"Weight\"]))\n",
    "print(min(demographics[\"Weight\"]))\n",
    "for i in demographics[\"Weight\"]:\n",
    "    if (i >= 100):\n",
    "        demographics[\"Weight\"].replace(i, 1.0, inplace=True)\n",
    "    elif (80 <= i < 100):\n",
    "        demographics[\"Weight\"].replace(i, 2.0, inplace=True)\n",
    "    elif (60 <= i < 80):\n",
    "        demographics[\"Weight\"].replace(i, 3.0, inplace=True)\n",
    "    elif (i < 60):\n",
    "        demographics[\"Weight\"].replace(i, 4.0, inplace=True)\n",
    "\n",
    "print(demographics[\"Number of siblings\"].unique())\n",
    "for i in demographics[\"Number of siblings\"]:\n",
    "    if i == 0:\n",
    "        demographics[\"Number of siblings\"].replace(i, 0.0, inplace=True)\n",
    "    elif i == 1:\n",
    "        demographics[\"Number of siblings\"].replace(i, 1.0, inplace=True)\n",
    "    elif i == 2:\n",
    "        demographics[\"Number of siblings\"].replace(i, 2.0, inplace=True)\n",
    "    elif i == 3:\n",
    "        demographics[\"Number of siblings\"].replace(i, 3.0, inplace=True)\n",
    "    elif i == 4:\n",
    "        demographics[\"Number of siblings\"].replace(i, 4.0, inplace=True)\n",
    "    elif i > 4:\n",
    "        demographics[\"Number of siblings\"].replace(i, 5.0, inplace=True)\n",
    "\n",
    "demographics = demographics.replace(\"nan\", np.nan)\n",
    "demographics = demographics.replace(\"NaN\", np.nan)\n",
    "\n",
    "imp.fit(demographics)\n",
    "demographics_data = imp.transform(demographics)\n",
    "demographics = pd.DataFrame(data=demographics_data[:,:],\n",
    "                     index=[i for i in range(len(demographics_data))],\n",
    "                     columns=demographics.columns.tolist())\n",
    "        \n",
    "data_all = music.join(movies.join(hobbies.join(phobias.join(health.join(\n",
    "            personality.join(spending.join(demographics)))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have to predict 'Empathy', we have to use the empathy column from the data as our prediction label Y and the remaining columns as our input X.\n",
    "\n",
    "Now that we have processed our data, we can use it make predictions.\n",
    "In order to do that we have to choose the best possible features from our data in order to avoid irrelevant information which can reduce the accuracy as well as increase the training time.\n",
    "Here we are selecting the best 100 features from the data by univariate selection using the chi2 score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting the best 100 features from the data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "print(\"Selecting the best 100 features from the data...\")\n",
    "y_all = [i for i in data_all.iloc[:, 95]]\n",
    "responses_train = data_all.drop([\"Empathy\"], axis=1)\n",
    "x_all = responses_train.iloc[:, :].values\n",
    "\n",
    "test = SelectKBest(score_func=chi2, k=100)\n",
    "bestfit = test.fit(x_all, y_all)\n",
    "features = bestfit.transform(x_all)\n",
    "#print(features.shape)\n",
    "x_all=features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected the best features, we have to split the data into training, development and testing data.\n",
    "We can split the data into 80% training and 20% testing data.\n",
    "We can further split the training data into 80% training which we can use to select a model and 20% development data to tune our model to make better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_all[:int(len(y_all)*0.8)]\n",
    "y_test = y_all[int(len(y_all)*0.8):]\n",
    "x = x_all[:int(len(x_all)*0.8)]\n",
    "x_test = x_all[int(len(x_all)*0.8):]\n",
    "\n",
    "x_train, x_dev, y_train, y_dev = model_selection.train_test_split(x, y, test_size=20, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train some basic models to see how they perform on the training data. \n",
    "We can train a classifier that predicts the most frequent class, a Logistic Regression model, SVM and KNN to see which of these achieves the best result on the training data.\n",
    "We can use a 10-fold cross validation using 'accuracy' as the scoring parameter and make our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating few classifiers on the training data using cross validation...\n",
      "accuracy for most frequent classifier:  36.28854268094774 %\n",
      "accuracy for stratified classifier:  36.28854268094774 %\n",
      "accuracy for SVM:  50.757870821161966 %\n",
      "accuracy for Logistic Regression:  71.44271340473874 %\n",
      "accuracy for KNN:  35.40246673158066 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "print(\"Evaluating few classifiers on the training data using cross validation...\")\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=50)\n",
    "pred = model_selection.cross_val_score(DummyClassifier(strategy='most_frequent'), x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "print(\"accuracy for most frequent classifier: \" , pred.mean()*100, \"%\")\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=50)\n",
    "pred = model_selection.cross_val_score(DummyClassifier(strategy='most_frequent'), x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "print(\"accuracy for stratified classifier: \" , pred.mean()*100, \"%\")\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=50)\n",
    "pred = model_selection.cross_val_score(SVC(), x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "print(\"accuracy for SVM: \" , pred.mean()*100, \"%\")\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=50)\n",
    "pred = model_selection.cross_val_score(LogisticRegression(), x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "print(\"accuracy for Logistic Regression: \" , pred.mean()*100, \"%\")\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=50)\n",
    "pred = model_selection.cross_val_score(KNeighborsClassifier(), x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "print(\"accuracy for KNN: \" , pred.mean()*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results, Logistic Regression achieved the best accuracy on the training data among all these models. So we can choose Logistic Regression as our model. \n",
    "Now we have to choose the best parameters for our Logistic Regression model to make the best possible predictions. To achieve this, we can use GridSearch to tune the hyperparameters on the validation/development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selcting Logistic Regression as our model because it achieved the highest accuracy...\n",
      "Tuning hyperparameters on the validation data..\n",
      "Accuracy on the development data:  95.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "print(\"Selcting Logistic Regression as our model because it achieved the highest accuracy...\")\n",
    "print(\"Tuning hyperparameters on the validation data..\")\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "C = np.logspace(0, 4, 5)\n",
    "multiclass=['ovr','multinomial']\n",
    "solver=['newton-cg','lbfgs','saga']\n",
    "hyperparameters = dict(C=C, multi_class=multiclass, solver=solver)\n",
    "GS = GridSearchCV(logistic, hyperparameters, cv=10, verbose=0)\n",
    "model = GS.fit(x_train, y_train)\n",
    "pred_dev = model.predict(x_dev)\n",
    "print(\"Accuracy on the development data: \",accuracy_score(y_dev, pred_dev)*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the hyperparameters that have achieved the best possible accuracy on the validation data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters found through GridSearch:  {'C': 10000.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'max_iter': 100, 'multi_class': 'multinomial', 'n_jobs': 1, 'penalty': 'l2', 'random_state': None, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "params = model.best_estimator_.get_params()\n",
    "print(\"best parameters found through GridSearch: \",params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these hyperparameters on the Logistic Regression model to make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on the test data...\n",
      "Accuracy on the test data:  91.0891089108911 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Making predictions on the test data...\")\n",
    "model = LogisticRegression(C=params['C'], multi_class=params['multi_class'], penalty=params['penalty'], solver=params['solver'])\n",
    "model.fit(x_train, y_train)\n",
    "pred_test = model.predict(x_test)\n",
    "print(\"Accuracy on the test data: \",accuracy_score(y_test, pred_test)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
